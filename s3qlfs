#!/usr/bin/env python
#
#    Copyright (C) 2008  Nikolaus Rath <Nikolaus@rath.org>
#
#    This program can be distributed under the terms of the GNU LGPL.
#

from optparse import OptionParser
from getpass  import getpass
import s3ql
import sys

#
# Parse command line
#
parser = OptionParser(
    usage="%prog  [options] <bucketname> <mountpoint>\n"
          "       %prog --help",
    description="Mounts an amazon S3 bucket as a filesystem.")

parser.add_option("--awskey", type="string",
                  help="Amazon Webservices access key to use. The password is "
                  "read from stdin. If this option is not specified, both access key "
                  "and password are read from ~/.awssecret (separated by newlines).")
parser.add_option("--debug", action="store_true", default=False,
                  help="Generate debugging output")
parser.add_option("--s3timeout", type="int", default=50,
                  help="Maximum time to wait for propagation in S3 (default: %default)")
parser.add_option("--allow_others", action="store_true", default=False,
                  help="Allow others users to access the filesystem")
parser.add_option("--allow_root", action="store_true", default=False,
                  help="Allow root to access the filesystem")
parser.add_option("--encrypt", action="store_true", default=None,
                  help="Mount an encrypted filesystem")
parser.add_option("--nonempty", action="store_true", default=False,
                  help="Allow mount if even mount point is not empty")
parser.add_option("--fg", action="store_true", default=False,
                  help="Do not daemonize, stay in foreground")

(options, pps) = parser.parse_args()

#
# Verify parameters
#
if not len(pps) == 2:
    parser.error("Wrong number of parameters")
bucketname = pps[0]
mountpoint = pps[1]


#
# Read password(s)
#
(awskey, awspass) = s3ql.get_credentials(options.awskey)

if options.encrypt:
    if sys.stdin.isatty():
        options.encrypt = getpass("Enter encryption password: ")
        if not options.encrypt == getpass("Confirm encryption password: "):
            print >>sys.stderr, "Passwords don't match."
            sys.exit(1)
    else:
        options.encrypt = sys.stdin.readline().rstrip()


#
# Pass on fuse options
#
fuse_opts = []
if options.allow_others:
    fuse_opts.append("allow_others")
if options.allow_root:
    fuse_opts.append("allow_root")
if options.nonempty:
    fuse_opts.append("nonempty")


#
# Activate logging
#
if options.debug:
    s3ql.log_level = 1


#
# Connect to S3
#
conn = s3ql.s3.Connection(awskey, awspass, options.encrypt)
bucket = conn.get_bucket(bucketname)

cachedir = s3ql.get_cachedir(bucketname)
dbfile = s3ql.get_dbfile(bucketname)

#
# Check consistency
#
s3ql.debug("Checking consistency...")
if bucket["dirty"] != "no":
    print >> sys.stderr, \
        "Metadata is dirty! Either some changes have not yet propagated\n" \
        "through S3 or the filesystem has not been umounted cleanly. In\n" \
        "the later case you should run s3fsck on the system where the\n" \
        "filesystem has been mounted most recently!\n"
    sys.exit(1)

# Init cache
if os.path.exists(cachedir) or os.path.exists(dbfile):
    print >> sys.stderr, \
        "Local cache files already exists! Either you are trying to\n" \
        "to mount a filesystem that is already mounted, or the filesystem\n" \
        "has not been umounted cleanly. In the later case you should run\n" \
        "s3fsck.\n"
    sys.exit(1)

# Init cache + get metadata
debug("Downloading metadata...")
os.mknod(dbfile, 0600 | stat.S_IFREG)
os.mkdir(cachedir, 0700)
bucket.fetch_to_file("metadata", dbfile)

# Check that the fs itself is clean
conn = apsw.Connection(dbfile)
(dirty,) = conn.cursor().execute("SELECT needs_fsck FROM parameters").next()
conn.close()
if dirty:
    print >> sys.stderr, "Filesystem damaged, run s3fsk!\n"
    os.unlink(dbfile)
    os.rmdir(cachedir)
    sys.exit(1)

#
# Start server
#
server = s3ql.fs(bucket, dbfile, cachepath)
server.main(mountpoint, fuse_opts, options.fg)
server.close()


# Upload database
debug("Uploading database..")
bucket.store_from_file("metadata", dbfile)
bucket.store("dirty", "no")

# Remove database
debug("Cleaning up...")
os.unlink(dbfile)
os.rmdir(cachedir)
